[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Welcome to My Blog!\nHello! This is Archer Liu, and welcome to my blog!\n\n\n\nThumbnail\n\n\nThis is where I’ll be posting updates on everything from personal reflections to the latest news and projects I’m working on. Stay tuned for more updates!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Archer’s Data Science Blog",
    "section": "",
    "text": "K-means clustering in a nutshell\n\n\n\n\n\n\ntutorial\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\nJan 18, 2025\n\n\nArcher Liu\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nJan 17, 2025\n\n\nArcher Liu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Archer Liu",
    "section": "",
    "text": "Still under constructions!"
  },
  {
    "objectID": "posts/k-means-in-a-nutshell/index.html",
    "href": "posts/k-means-in-a-nutshell/index.html",
    "title": "K-means clustering in a nutshell",
    "section": "",
    "text": "Have you ever wondered how Netflix seems to know exactly what show you’ll love next? Or how Amazon always recommends products you didn’t even realize you needed until it pops up on your home page? How do they get so smart and know you so well?\nThese smart recommendations aren’t because your devices are eavesdropping on you…well, maybe they do. But! They’re actually powered by amazing data science techniques like K-means clustering!\nIn this tutorial, we’ll introduce the magic of K-means clustering in a nutshell, exploring how it works and how it can help you play around with everyday data you’re interested in, from analyzing social media trends, to enhancing your learning projects. (Or just for fun!)"
  },
  {
    "objectID": "posts/k-means-in-a-nutshell/index.html#so-what-is-k-means-clustering",
    "href": "posts/k-means-in-a-nutshell/index.html#so-what-is-k-means-clustering",
    "title": "K-means clustering in a nutshell",
    "section": "So, what is K-means clustering?",
    "text": "So, what is K-means clustering?\nBefore diving into writing fancy code to explore our example dataset, let’s familiarize ourselves with some key terminology.\nFirst, what is clustering? According to wikipedia, it is the process of dividing a set of objects into groups (called clusters), where objects within the same group are more similar to each other than to objects in other groups (Wikipedia, 2025). Just as people naturally seek out others with shared interests and form small social circles, clustering algorithms group data points based on their shared features.\n\n\n\nDifferent animals can be clustered based on their features, and probably, different memes!\n\n\nThe K-means clustering is one of the most used clustering algorithms. The “K” stands for the number of groups you choose. “Means” comes from finding the average (or center) of each group. So basically, K-means clustering groups data into K clusters by assigning each point to the closest center, using straight-line distance between the point and the center. Then, it recalculate the center of each group. The new center is the average of all the points in that group. The process keeps repeating until all the centers and groups are as stable as possible. Easy peasy right?\n\n\n\nK-means clustering precedures\n\n\nHowever, there’s one more challenge: choosing the right K. If you select too few groups, the groups may be too broad; if you pick too many, the groups may become overly specific. So, how do we find the best number? This is where the Elbow Method comes to the rescue!\nThe Elbow Method helps find the best number of clusters by looking at how the Sum of Squared Errors (SSE, a measure of how far the data points are from their group centers) changes as K increases. When K is small, increasing it reduces the error a lot. But after a certain point, increasing K doesn’t reduce the error much. The K value where this happens will be the sweet spot.\nBut there’s a trade-off. If we pick a really large K, we might get too many groups. This could break the data into tiny, unhelpful segments, making it harder to find useful patterns. So, it’s important to choose the right K, so that we can have enough groups to be useful, but not so many that they become meaningless. We’ll explore more about this in the coding section."
  },
  {
    "objectID": "posts/k-means-in-a-nutshell/index.html#time-to-code",
    "href": "posts/k-means-in-a-nutshell/index.html#time-to-code",
    "title": "K-means clustering in a nutshell",
    "section": "Time to code!",
    "text": "Time to code!\nNow that we’ve gotten the hang of the core concept of K-means, what better way to really get it than giving it a go ourselves?\nWe’ll be using some handy Python packages: pandas, matplotlib, and scikit-learn. Make sure you have them installed! If not, you can easily install them via the command line using pip:\npip install pandas matplotlib scikit-learn\nIn this practice, we’ll work with a dataset of mall customers (Sudheer, 2022). You can download the dataset here. This dataset contains some interesting facts about the customers, including their CustomerID, Gender, Age, Annual Income (k$), and Spending Score (1-100) (a score based on customer behavior and purchasing habits). As the data scientist for the company, your task today is to identify distinct customer profiles via K-means clustering, so that the mall can make wiser decisions about which products to offer and which ads to display for the right customers.\nAfter downloading the dataset Mall_Customers.csv into the current directory, we’ll load the data and clean it up a bit, such as removing any irrelevant properties for clustering.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\n\n# Read the data and print some head rows\ndata = pd.read_csv(\"Mall_Customers.csv\")\nprint(data.head(5))\n\n# Drop the CustomerID column and print some head rows\ndata_drop_ID = data.drop(\"CustomerID\", axis=1)\nprint(data_drop_ID.head(5))\n\n   CustomerID  Gender  Age  Annual Income (k$)  Spending Score (1-100)\n0           1    Male   19                  15                      39\n1           2    Male   21                  15                      81\n2           3  Female   20                  16                       6\n3           4  Female   23                  16                      77\n4           5  Female   31                  17                      40\n   Gender  Age  Annual Income (k$)  Spending Score (1-100)\n0    Male   19                  15                      39\n1    Male   21                  15                      81\n2  Female   20                  16                       6\n3  Female   23                  16                      77\n4  Female   31                  17                      40\n\n\nIt looks like our data still needs a little makeover. Some columns contain text, while others have numeric values with different ranges. This can make it tricky when calculating distances for clustering. To resolve this, we’ll need to encode the text data and scale the numeric data so that everything is on the same level before clustering.\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nlabel_encoder = LabelEncoder()\nscaler = StandardScaler()\n\n# Encode the 'Gender' column, turning \"Male\" and \"Female\" into numbers 1 and 0\ndata_drop_ID['Gender'] = label_encoder.fit_transform(data_drop_ID['Gender'])\n\n# Scale the features so they're on the same scale\ndata_scaled = scaler.fit_transform(data_drop_ID[data_drop_ID.columns])\n\n# The cleaned dataset\nfinal_data = pd.DataFrame(data_scaled, columns=data_drop_ID.columns)\nprint(final_data.head(5))\n\n     Gender       Age  Annual Income (k$)  Spending Score (1-100)\n0  1.128152 -1.424569           -1.738999               -0.434801\n1  1.128152 -1.281035           -1.738999                1.195704\n2 -0.886405 -1.352802           -1.700830               -1.715913\n3 -0.886405 -1.137502           -1.700830                1.040418\n4 -0.886405 -0.563369           -1.662660               -0.395980\n\n\nFinally, We can start with K-means! Keep the basic steps in mind as we loop through different K values and use the elbow method to find the best K. Let’s get clustering!\n\n# Convert the data into a format suitable for clustering\nX = final_data.values\n\n# Create a list to store the Sum of Squared Errors for each K value\nsumDs = []\n\n# Try different values of K\nfor i in range(1, 20):\n    kmeans = KMeans(n_clusters=i)  # Create a KMeans model with K clusters\n    kmeans.fit(X)  # Fit the model to our data, grouping the data points into K clusters\n    sumDs.append(kmeans.inertia_)  # Add the SSE for the current K to the list\n\n# Plot the Elbow plot to find a reasonable K\nplt.plot(range(1, 20), sumDs)\nplt.title('Elbow Plot for Optimal K')\nplt.xlabel('Number of Clusters (K)')\nplt.ylabel('Sum of Squared Errors (SSE)')\nplt.show()\n\n\n\n\n\n\n\n\nYou’ll probably see that K=20 gives the smallest SSE, but remember, too many clusters might make the results meaningless! So instead of picking the K that gives the smallest error, we might go with K=6, where the error still decreases but the rate of improvement slows down a lot.\nAnd here it is, the moment of truth. Let’s see how our clustering results look with K=6 in 3D!\n\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Fit KMeans with K=6\nkmeans = KMeans(n_clusters=6)\nsegmentation = kmeans.fit_predict(final_data)\nfinal_data['Segmentation'] = segmentation\n\n# Create a 3D plot\nfig = plt.figure(figsize=(10, 8))\nax = fig.add_subplot(111, projection='3d')\nscatter = ax.scatter(\n    final_data['Age'], \n    final_data['Annual Income (k$)'], \n    final_data['Spending Score (1-100)'], \n    c=final_data['Segmentation'], \n    cmap='viridis'\n)\nax.set_xlabel('Age')\nax.set_ylabel('Annual Income (k$)')\nax.set_zlabel('Spending Score (1-100)')\nax.set_title('Mall Customer Segmentation with 6 clusters')\nplt.colorbar(scatter)\n\nplt.show()"
  },
  {
    "objectID": "posts/k-means-in-a-nutshell/index.html#wrap-up",
    "href": "posts/k-means-in-a-nutshell/index.html#wrap-up",
    "title": "K-means clustering in a nutshell",
    "section": "Wrap up!",
    "text": "Wrap up!\nAnd there you go! We’ve just built our very first K-means clustering project! Pretty neat, right?\nLet’s quickly recap what we’ve learned. K-means clustering is a powerful technique for grouping similar data points and performing segmentation. We’ve covered the fundamentals, including the steps involved in K-means clustering and how the Elbow Method can help us determine the optimal number of clusters.\nHowever, there are several important aspects of K-means clustering that we haven’t covered. For instance, K-means can be easily affected by outliers (abnormal data points with extreme values), which may lead to poor clustering. Exploring ways to address the outliers can help improve results. Additionally, there are ways other than the elbow method that can help check how good our clusters are, such as Silhouette Coefficient, which checks how similar items are within a group.\nThis is just the first steps into the world of unsupervised learning, and there’s so much more to explore! Don’t hesitate to apply what you’ve learned to your own ideas! And of course, stay tuned for more exciting sharings!"
  },
  {
    "objectID": "posts/k-means-in-a-nutshell/index.html#references",
    "href": "posts/k-means-in-a-nutshell/index.html#references",
    "title": "K-means clustering in a nutshell",
    "section": "References",
    "text": "References\n\nWikipedia contributors. 2025. “Cluster Analysis.” Wikipedia. Retrieved from https://en.wikipedia.org/wiki/Cluster_analysis.\nNelakurthi Sudheer. 2022. Mall Customer Segmentation, Version 1. Retrieved from https://www.kaggle.com/datasets/nelakurthisudheer/mall-customer-segmentation/data.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, E. (2011). Scikit-learn: Machine learning in Python. Journal of Machine Learning Research, 12, 2825–2830.\nThe pandas development team. (2020). pandas-dev/pandas: Pandas (Version latest). Zenodo. https://doi.org/10.5281/zenodo.3509134\nHunter, J. D. (2007). Matplotlib: A 2D graphics environment. Computing in Science & Engineering, 9(3), 90–95. https://doi.org/10.1109/MCSE.2007.55"
  }
]